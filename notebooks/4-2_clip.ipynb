{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# CLIP の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/py-img-gen/python-image-generation/blob/main/notebooks/4-2_clip.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq py-img-gen[clip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "dtype = torch.float16\n",
    "seed = 42\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## CLIP の動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### CLIP モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "model_id = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "# CLIP モデルの読み込み\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "# モデルを推論モードにする\n",
    "# このとき dropout を無効化したり、batch normalization の動作を推論用にする\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### CLIP 用の前処理 pipeline の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### CLIP のパラメータ情報の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_params = sum(\n",
    "    [int(np.prod(p.shape)) for p in model.parameters()]\n",
    ")\n",
    "input_resolution = model.config.vision_config.image_size\n",
    "context_length = processor.tokenizer.model_max_length\n",
    "num_vocab = model.config.text_config.vocab_size\n",
    "\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"Input resolution: {input_resolution}\")\n",
    "print(f\"Context length: {context_length}\")\n",
    "print(f\"Vocab size: {num_vocab:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## CLIPProcessor の動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### サンプル画像のダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "image = load_image(\n",
    "    \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/000002025.png\"\n",
    ")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### CLIPProcessor による画像の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = processor(images=image, return_tensors=\"pt\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"pixel_values\"].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### CLIPProcessor によるテキストの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = processor(text=\"Hello world\", return_tensors=\"pt\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.batch_decode(output[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## CLIP による画像とテキストの類似度計算\n",
    "\n",
    "- 参考: https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### zero-shot 分類用のプロンプトの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 使用する skimage の画像とその説明文\n",
    "#\n",
    "descriptions_dict = {\n",
    "    \"page\": \"a page of text about segmentation\",\n",
    "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "    \"rocket\": \"a rocket standing on a launchpad\",\n",
    "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
    "    \"camera\": \"a person looking at a camera on a tripod\",\n",
    "    \"horse\": \"a black-and-white silhouette of a horse\",\n",
    "    \"coffee\": \"a cup of coffee on a saucer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 画像とテキストのペアの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import skimage\n",
    "from more_itertools import sort_together\n",
    "\n",
    "original_imgs, original_txts = [], []\n",
    "\n",
    "# skimage から .png か .jpg な画像のパスを習得する\n",
    "filenames = [\n",
    "    fname\n",
    "    for fname in os.listdir(skimage.data_dir)\n",
    "    if fname.endswith(\".png\") or fname.endswith(\".jpg\")\n",
    "]\n",
    "for fname in filenames:\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    if name not in descriptions_dict:\n",
    "        continue\n",
    "\n",
    "    # 画像の読み込み\n",
    "    image_path = os.path.join(skimage.data_dir, fname)\n",
    "    original_imgs.append(load_image(image_path))\n",
    "    # テキストの読み込み\n",
    "    original_txts.append(descriptions_dict[name])\n",
    "\n",
    "# 画像とテキストの数があっているか確認\n",
    "assert len(original_txts) == len(original_imgs)\n",
    "\n",
    "# テキストの文字列をベースに、テキストと画像のリストをソートする\n",
    "original_txts, original_imgs = sort_together(\n",
    "    (original_txts, original_imgs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 画像とテキストのペアの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nrows, ncols = 2, 4\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, figsize=(16, 5)\n",
    ")\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        axes[i][j].imshow(original_imgs[i * ncols + j])\n",
    "        axes[i][j].axis(\"off\")\n",
    "        axes[i][j].set_title(\n",
    "            original_txts[i * ncols + j], fontsize=10\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 画像とテキストのペアの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=original_txts,\n",
    "    images=original_imgs,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### CLIP による画像とテキストの特徴の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features = model.get_image_features(\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "    )\n",
    "    txt_features = model.get_text_features(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 画像とテキストの類似度計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = img_features / img_features.norm(\n",
    "    p=2, dim=-1, keepdim=True\n",
    ")\n",
    "txt_features = txt_features / txt_features.norm(\n",
    "    p=2, dim=-1, keepdim=True\n",
    ")\n",
    "\n",
    "similarity = img_features @ txt_features.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 画像とテキストの類似度の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(original_imgs) == len(original_txts)\n",
    "count = len(original_imgs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 14))\n",
    "ax.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "\n",
    "ax.set_yticks(\n",
    "    range(len(original_txts)),\n",
    "    labels=original_txts,\n",
    "    fontsize=18,\n",
    ")\n",
    "ax.set_xticks([])\n",
    "\n",
    "for i, img in enumerate(original_imgs):\n",
    "    extent = (i - 0.5, i + 0.5, -1.6, -0.6)\n",
    "    ax.imshow(img, extent=extent, origin=\"lower\")\n",
    "\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        s = f\"{similarity[y, x]:.2f}\"\n",
    "        a = \"center\"\n",
    "        ax.text(x, y, s=s, ha=a, va=a, size=12)\n",
    "\n",
    "for side in (\"left\", \"top\", \"right\", \"bottom\"):\n",
    "    plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "ax.set_xlim((-0.5, count - 0.5))\n",
    "ax.set_ylim((count - 0.5, -2))\n",
    "\n",
    "ax.set_title(\n",
    "    \"Cosine similarity between text and image features\",\n",
    "    size=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## CLIP による zero-shot 画像分類"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### CIFAR100 データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "cifar100 = CIFAR100(\n",
    "    os.path.expanduser(\"~/.cache\"), download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### プロンプトの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_template = \"This is a photo of a {label}\"\n",
    "text_descriptions = [\n",
    "    text_template.format(label=label)\n",
    "    for label in cifar100.classes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### テキスト特徴の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=text_descriptions,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    txt_features = model.get_text_features(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "    )\n",
    "    txt_features = txt_features / txt_features.norm(\n",
    "        p=2, dim=-1, keepdim=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 類似度を下にした分類結果の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_probs = 100 * img_features @ txt_features.T\n",
    "txt_probs = txt_probs.softmax(dim=-1)\n",
    "top_probs, top_labels = txt_probs.topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### zero-shot 分類結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 4, 4\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "y = np.arange(top_probs.shape[-1])\n",
    "\n",
    "for i, img in enumerate(original_imgs):\n",
    "    ax1 = fig.add_subplot(nrows, ncols, 2 * i + 1)\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(original_txts[i], fontsize=10)\n",
    "\n",
    "    ax2 = fig.add_subplot(nrows, ncols, 2 * i + 2)\n",
    "    ax2.barh(y, top_probs[i])\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    ax2.set_yticks(\n",
    "        y, [cifar100.classes[idx] for idx in top_labels[i]]\n",
    "    )\n",
    "    ax2.set_xlabel(\"Probability\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
