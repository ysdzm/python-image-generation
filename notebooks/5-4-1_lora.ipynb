{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LoRA の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/py-img-gen/python-image-generation/blob/main/notebooks/5-4-1_lora.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq py-img-gen[lora,quantization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "current_dir = pathlib.Path.cwd()\n",
    "project_dir = current_dir / \"data\" / \"lora\"\n",
    "project_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Created a directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"text_encoder\",\n",
    ")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"vae\",\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"unet\",\n",
    ")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"scheduler\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig(object):\n",
    "    name: str = \"py-img-gen/ukiyo-e-face-blip2-captions\"\n",
    "    config_name: Optional[str] = None\n",
    "    cache_dir: Optional[str] = None\n",
    "    image_column: str = \"image\"\n",
    "    caption_column: str = \"caption\"\n",
    "\n",
    "\n",
    "dataset_config = DatasetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "\n",
    "dataset = ds.load_dataset(\n",
    "    path=dataset_config.name,\n",
    "    name=dataset_config.config_name,\n",
    "    cache_dir=dataset_config.cache_dir,\n",
    ")\n",
    "assert isinstance(dataset, ds.DatasetDict)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameter(object):\n",
    "    seed: int = 19950815\n",
    "    num_train_epochs: int = 100\n",
    "    learning_rate: float = 1e-4\n",
    "    is_scale_lr: bool = False\n",
    "    lr_scheduler: str = \"constant\"\n",
    "    lr_warmup_steps: int = 0\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    gradient_checkpointing: bool = True\n",
    "    max_grad_norm: float = 1.0\n",
    "    mixed_precision: str = \"fp16\"\n",
    "    lora_rank: int = 4\n",
    "    image_size: int = 512\n",
    "    center_crop: bool = False\n",
    "    random_flip: bool = False\n",
    "    train_batch_size: int = 16\n",
    "    save_steps: int = 500\n",
    "    output_dir_path: pathlib.Path = project_dir\n",
    "\n",
    "    # `bitsandbytes` による 8-bit 最適化を利用する場合は `True` にしてください\n",
    "    use_8bit_adam: bool = True\n",
    "\n",
    "\n",
    "hparams = Hyperparameter()\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "\n",
    "    captions = []\n",
    "    for caption in examples[dataset_config.caption_column]:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            # take a random caption if there are multiple\n",
    "            captions.append(\n",
    "                random.choice(caption)\n",
    "                if is_train\n",
    "                else caption[0]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Caption column `{dataset_config.caption_column}` \"\n",
    "                \"should contain either strings or lists of strings.\"\n",
    "            )\n",
    "    inputs = tokenizer(\n",
    "        captions,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            hparams.image_size,\n",
    "            interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "        ),\n",
    "        transforms.CenterCrop(hparams.image_size)\n",
    "        if hparams.center_crop\n",
    "        else transforms.RandomCrop(hparams.image_size),\n",
    "        transforms.RandomHorizontalFlip()\n",
    "        if hparams.random_flip\n",
    "        else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def preprocess_train(examples):\n",
    "    images = [\n",
    "        image.convert(\"RGB\")\n",
    "        for image in examples[dataset_config.image_column]\n",
    "    ]\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transforms(image) for image in images\n",
    "    ]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"].with_transform(\n",
    "    preprocess_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"pixel_values\"] for example in examples]\n",
    "    )\n",
    "    pixel_values = pixel_values.to(\n",
    "        memory_format=torch.contiguous_format\n",
    "    ).float()\n",
    "    input_ids = torch.stack(\n",
    "        [example[\"input_ids\"] for example in examples]\n",
    "    )\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "unet.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "if hparams.gradient_checkpointing:\n",
    "    unet.enable_gradient_checkpointing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import cast_training_params\n",
    "from diffusers.utils import convert_state_dict_to_diffusers\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "from peft import LoraConfig\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def training_function(\n",
    "    text_encoder: CLIPTextModel,\n",
    "    vae: AutoencoderKL,\n",
    "    unet: UNet2DConditionModel,\n",
    "):\n",
    "    # 学習の再現性を確保するために乱数の seed を固定\n",
    "    set_seed(hparams.seed)\n",
    "\n",
    "    # 学習を効率化する Accelerator の設定\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=hparams.gradient_accumulation_steps,\n",
    "        mixed_precision=hparams.mixed_precision,\n",
    "    )\n",
    "    print(accelerator.state)\n",
    "\n",
    "    # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision\n",
    "    # as these weights are only used for inference, keeping weights in full precision is not required.\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid dtype: {accelerator.mixed_precision}\"\n",
    "        )\n",
    "\n",
    "    # Freeze the unet parameters before adding adapters\n",
    "    for param in unet.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    # Move unet, vae and text_encoder to device and cast to weight_dtype\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # Add adapter and make sure the trainable params are in float32.\n",
    "    unet_lora_config = LoraConfig(\n",
    "        r=hparams.lora_rank,\n",
    "        lora_alpha=hparams.lora_rank,\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    )\n",
    "    unet.add_adapter(unet_lora_config)\n",
    "    if hparams.mixed_precision == \"fp16\":\n",
    "        # only upcast trainable parameters (LoRA) into fp32\n",
    "        cast_training_params(unet, dtype=torch.float32)\n",
    "\n",
    "    # Filter out parameters for LoRA layers\n",
    "    lora_layers = list(\n",
    "        filter(lambda p: p.requires_grad, unet.parameters())\n",
    "    )\n",
    "\n",
    "    # 学習率をハイパーパラメータを考慮してスケーリングする\n",
    "    learning_rate = (\n",
    "        (\n",
    "            hparams.learning_rate\n",
    "            * hparams.gradient_accumulation_steps\n",
    "            * hparams.train_batch_size\n",
    "            * accelerator.num_processes\n",
    "        )\n",
    "        if hparams.is_scale_lr\n",
    "        else hparams.learning_rate\n",
    "    )\n",
    "\n",
    "    # Colab の T5 GPU のような、16 GB 以下の GPU RAM の場合は\n",
    "    # fine-tuning 時のメモリ使用量を減らすために 8-bit の Adam optimizer を使用\n",
    "    if hparams.use_8bit_adam:\n",
    "        import bitsandbytes as bnb\n",
    "\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    optimizer = optimizer_class(\n",
    "        lora_layers,\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=hparams.train_batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    # Check the PR https://github.com/huggingface/diffusers/pull/8312 for detailed explanation.\n",
    "    num_warmup_steps_for_scheduler = (\n",
    "        hparams.lr_warmup_steps * accelerator.num_processes\n",
    "    )\n",
    "    len_train_dataloader_after_sharding = math.ceil(\n",
    "        len(train_data_loader) / accelerator.num_processes\n",
    "    )\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len_train_dataloader_after_sharding\n",
    "        / hparams.gradient_accumulation_steps\n",
    "    )\n",
    "    num_training_steps_for_scheduler = (\n",
    "        hparams.num_train_epochs\n",
    "        * num_update_steps_per_epoch\n",
    "        * accelerator.num_processes\n",
    "    )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        hparams.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps_for_scheduler,\n",
    "        num_training_steps=num_training_steps_for_scheduler,\n",
    "    )\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    unet, optimizer, train_data_loader, lr_scheduler = (\n",
    "        accelerator.prepare(\n",
    "            unet, optimizer, train_data_loader, lr_scheduler\n",
    "        )\n",
    "    )\n",
    "\n",
    "    max_train_steps = (\n",
    "        hparams.num_train_epochs\n",
    "        * num_update_steps_per_epoch\n",
    "    )\n",
    "    hparams.num_train_epochs = math.ceil(\n",
    "        max_train_steps / num_update_steps_per_epoch\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = (\n",
    "        hparams.train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * hparams.gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    print(\"***** Running training *****\")\n",
    "    print(f\"  Num examples = {len(train_dataset)}\")\n",
    "    print(f\"  Num Epochs = {hparams.num_train_epochs}\")\n",
    "    print(\n",
    "        f\"  Instantaneous batch size per device = {hparams.train_batch_size}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Gradient Accumulation steps = {hparams.gradient_accumulation_steps}\"\n",
    "    )\n",
    "    print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "\n",
    "    def unwrap_model(model):\n",
    "        model = accelerator.unwrap_model(model)\n",
    "        model = (\n",
    "            model._orig_mod\n",
    "            if is_compiled_module(model)\n",
    "            else model\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        range(0, max_train_steps),\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "        desc=\"Steps\",\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    train_data_iter = iter(train_data_loader)\n",
    "\n",
    "    while global_step != max_train_steps:\n",
    "        try:\n",
    "            batch = next(train_data_iter)\n",
    "        except StopIteration:\n",
    "            train_data_iter = iter(train_data_loader)\n",
    "            batch = next(train_data_iter)\n",
    "\n",
    "        with accelerator.accumulate(unet):\n",
    "            loss = train_step(\n",
    "                batch=batch,\n",
    "                text_encoder=text_encoder,\n",
    "                vae=vae,\n",
    "                unet=unet,\n",
    "                noise_scheduler=noise_scheduler,\n",
    "                lora_layers=lora_layers,\n",
    "                weight_dtype=weight_dtype,\n",
    "                accelerator=accelerator,\n",
    "                optimizer=optimizer,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "            )\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % hparams.save_steps == 0:\n",
    "                unwrapped_unet = unwrap_model(unet)\n",
    "                unet_lora_state_dict = (\n",
    "                    convert_state_dict_to_diffusers(\n",
    "                        get_peft_model_state_dict(\n",
    "                            unwrapped_unet\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                StableDiffusionPipeline.save_lora_weights(\n",
    "                    save_directory=hparams.output_dir_path,\n",
    "                    unet_lora_layers=unet_lora_state_dict,\n",
    "                )\n",
    "                print(\n",
    "                    f\"Saved state to {hparams.output_dir_path}\"\n",
    "                )\n",
    "\n",
    "        logs = {\"loss\": loss.detach().item()}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        unet = unet.to(torch.float32)\n",
    "        unwrapped_unet = unwrap_model(unet)\n",
    "        unet_lora_state_dict = (\n",
    "            convert_state_dict_to_diffusers(\n",
    "                get_peft_model_state_dict(unwrapped_unet)\n",
    "            )\n",
    "        )\n",
    "        StableDiffusionPipeline.save_lora_weights(\n",
    "            save_directory=hparams.output_dir_path,\n",
    "            unet_lora_layers=unet_lora_state_dict,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    text_encoder: CLIPTextModel,\n",
    "    vae: AutoencoderKL,\n",
    "    unet: UNet2DConditionModel,\n",
    "    noise_scheduler: DDPMScheduler,\n",
    "    lora_layers: List[nn.Parameter],\n",
    "    weight_dtype: torch.dtype,\n",
    "    accelerator: Accelerator,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.LambdaLR,\n",
    ") -> torch.Tensor:\n",
    "    pixel_values = batch[\"pixel_values\"].to(\n",
    "        dtype=weight_dtype\n",
    "    )\n",
    "    latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "    latents *= vae.config.scaling_factor\n",
    "\n",
    "    noise = torch.randn_like(latents)\n",
    "    bsz = latents.shape[0]\n",
    "    timesteps = torch.randint(\n",
    "        0,\n",
    "        noise_scheduler.config.num_train_timesteps,\n",
    "        (bsz,),\n",
    "        device=latents.device,\n",
    "    )\n",
    "    timesteps = timesteps.long()\n",
    "\n",
    "    noisy_latents = noise_scheduler.add_noise(\n",
    "        latents, noise, timesteps\n",
    "    )\n",
    "\n",
    "    # Get the text embedding for conditioning\n",
    "    encoder_hidden_states = text_encoder(\n",
    "        batch[\"input_ids\"]\n",
    "    )[0]\n",
    "\n",
    "    if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "        target = noise\n",
    "    elif (\n",
    "        noise_scheduler.config.prediction_type\n",
    "        == \"v_prediction\"\n",
    "    ):\n",
    "        target = noise_scheduler.get_velocity(\n",
    "            latents, noise, timesteps\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unknown prediction type {noise_scheduler.config.prediction_type}\"\n",
    "        )\n",
    "\n",
    "    # Predict the noise residual and compute loss\n",
    "    model_pred = unet(\n",
    "        noisy_latents,\n",
    "        timesteps,\n",
    "        encoder_hidden_states,\n",
    "    )[0]\n",
    "\n",
    "    loss = F.mse_loss(\n",
    "        model_pred.float(), target.float(), reduction=\"mean\"\n",
    "    )\n",
    "\n",
    "    # Backpropagationを実施\n",
    "    accelerator.backward(loss)\n",
    "\n",
    "    if accelerator.sync_gradients:\n",
    "        params_to_clip = lora_layers\n",
    "        accelerator.clip_grad_norm_(\n",
    "            params_to_clip, hparams.max_grad_norm\n",
    "        )\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import accelerate\n",
    "\n",
    "accelerate.notebook_launcher(\n",
    "    training_function,\n",
    "    args=(text_encoder, vae, unet),\n",
    "    num_processes=1,\n",
    ")\n",
    "\n",
    "for param in itertools.chain(\n",
    "    unet.parameters(), text_encoder.parameters()\n",
    "):\n",
    "    if param.grad is not None:\n",
    "        # Colab では RAM の制約があるため勾配に関する情報を削除\n",
    "        del param.grad\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "pipe.unet.load_attn_procs(hparams.output_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mona Lisa\"\n",
    "\n",
    "output = pipe(\n",
    "    prompt=prompt,\n",
    "    generator=torch.manual_seed(hparams.seed),\n",
    ")\n",
    "image = output.images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
