{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Stable Diffusion を構成する要素を用いた実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/py-img-gen/python-image-generation/blob/main/notebooks/4-3_stable-diffusion_components.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "参考: https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq py-img-gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "dtype = torch.float16\n",
    "variant = \"fp16\"\n",
    "seed = 42\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 読み込むモデル名を指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "clip_model_id = \"openai/clip-vit-large-patch14\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Image Encoder である VAE モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    sd_model_id,\n",
    "    subfolder=\"vae\",\n",
    ")\n",
    "vae = vae.to(device)\n",
    "\n",
    "scale_factor = vae.config.scaling_factor\n",
    "image_processor = VaeImageProcessor(\n",
    "    vae_scale_factor=scale_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Denoiser である UNet モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    sd_model_id,\n",
    "    subfolder=\"unet\",\n",
    ")\n",
    "unet = unet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Text encoder である CLIP モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(clip_model_id)\n",
    "text_encoder = text_encoder.to(device)\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Noise scheduler である LMSDiscreteScheduler の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "scheduler = LMSDiscreteScheduler.from_pretrained(\n",
    "    sd_model_id, subfolder=\"scheduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 画像生成する際のハイパーパラメータの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a horse\"\n",
    "\n",
    "width = height = 512  # 生成画像の幅と高さ\n",
    "\n",
    "# ノイズ除去のステップ数; デフォルト値を使用\n",
    "num_inference_steps = 50\n",
    "# classifier-free guidance の guidance scale\n",
    "guidance_scale = 7.5\n",
    "# 乱数生成器に指定されたシード値を設定\n",
    "generator = torch.manual_seed(seed)\n",
    "# バッチサイズ\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## テキスト条件埋め込みの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = text_encoder(text_input.input_ids.to(device))\n",
    "    text_embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(text_embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## テキスト条件無し埋め込みの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size,\n",
    "    padding=\"max_length\",\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = text_encoder(\n",
    "        uncond_input.input_ids.to(device)\n",
    "    )\n",
    "    uncond_embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(uncond_embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 2つの条件埋め込みの結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.cat(\n",
    "    [uncond_embeddings, text_embeddings]\n",
    ")\n",
    "\n",
    "print(text_embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 潜在表現（ノイズ画像）の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = (\n",
    "    batch_size,\n",
    "    unet.config.in_channels,\n",
    "    height // 8,\n",
    "    width // 8,\n",
    ")\n",
    "latents = torch.randn(\n",
    "    size=latent_size,\n",
    "    generator=generator,\n",
    ")\n",
    "latents = latents.to(device)\n",
    "print(latents.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Noise scheduler の設定 とそれに対応した潜在表現の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "latents *= scheduler.init_noise_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 逆拡散過程を用いたノイズ除去による潜在表現の生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reverse_diffusion_process(\n",
    "    latents: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    # Classifier-free guidance で 2 回 のモデル forward 計算を\n",
    "    # 避けるために、潜在表現を 2 つににしてバッチ化します\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler.scale_model_input(\n",
    "        latent_model_input, t\n",
    "    )\n",
    "\n",
    "    # U-Net を元にノイズ残差を予測します\n",
    "    noise_pred = unet(\n",
    "        latent_model_input,\n",
    "        t,\n",
    "        encoder_hidden_states=text_embeddings,\n",
    "    ).sample\n",
    "\n",
    "    # Classifier-free guidance を適用します\n",
    "    # - 計算されたノイズ残差に対して、無条件/条件付き埋め込みに分割\n",
    "    # - 分割されたそれぞれを用いて classifier-free guidance を計算\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "        noise_pred_text - noise_pred_uncond\n",
    "    )\n",
    "\n",
    "    # 現在のステップ x_t から前のステップ x_{t-1} を予測\n",
    "    latents = scheduler.step(\n",
    "        noise_pred, t, latents\n",
    "    ).prev_sample\n",
    "\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    latents = reverse_diffusion_process(latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 生成された潜在表現のデコードと生成結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "with torch.no_grad():\n",
    "    images = vae.decode(\n",
    "        latents / scale_factor,\n",
    "        generator=torch.manual_seed(seed),\n",
    "    ).sample\n",
    "\n",
    "image_processor = VaeImageProcessor(\n",
    "    vae_scale_factor=scale_factor\n",
    ")\n",
    "images = image_processor.postprocess(images)\n",
    "\n",
    "images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
